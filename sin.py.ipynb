{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de291ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8e999475d12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 20\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 1,\n",
    "        output_size: int = 1,\n",
    "        hidden_dim: int = 32,\n",
    "        n_layers: int = 4\n",
    "    ) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x: Tensor, hidden: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = r_out.view(-1, self.hidden_dim)  \n",
    "        \n",
    "        output = self.fc(r_out)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5481037a64f6756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T11:14:55.745765Z",
     "start_time": "2025-02-22T11:14:55.737738Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    *,\n",
    "    rnn: nn.Module,\n",
    "    n_steps: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.MSELoss,\n",
    "    seq_length: int,\n",
    "    print_every: int | None = None,\n",
    ") -> tuple[nn.Module, list[float]]:\n",
    "    rnn.to(DEVICE)\n",
    "    losses = []\n",
    "    hidden = None\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        time_steps = np.linspace(step * np.pi, step * np.pi + 10, seq_length + 1)\n",
    "        data = np.sin(time_steps)\n",
    "        data.resize((seq_length + 1, 1))\n",
    "\n",
    "        x = data[:-1]\n",
    "        y = data[1:]\n",
    "\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0).to(DEVICE)\n",
    "        y_tensor = torch.Tensor(y).to(DEVICE)\n",
    "\n",
    "        prediction, hidden = rnn(x_tensor, hidden)\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        loss = criterion(prediction, y_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if print_every is not None and (step % print_every == 0):\n",
    "            print('Loss: ', loss.item())\n",
    "            plt.plot(time_steps[1:], x, 'r.')\n",
    "            plt.plot(time_steps[1:], prediction.cpu().data.numpy().flatten(), 'b.')\n",
    "            plt.show()\n",
    "\n",
    "    return rnn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19c2178f0d74137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T11:20:11.850516Z",
     "start_time": "2025-02-22T11:16:25.347852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... hidden dim: 32, Layers: 1, LR: 0.01, Steps: 99\n",
      "... hidden dim: 32, Layers: 1, LR: 0.001, Steps: 999\n",
      "... hidden dim: 32, Layers: 1, LR: 0.0001, Steps: 9999\n",
      "... hidden dim: 32, Layers: 2, LR: 0.01, Steps: 99\n",
      "... hidden dim: 32, Layers: 2, LR: 0.001, Steps: 999\n",
      "... hidden dim: 32, Layers: 2, LR: 0.0001, Steps: 9999\n",
      "... hidden dim: 32, Layers: 4, LR: 0.01, Steps: 99\n",
      "... hidden dim: 32, Layers: 4, LR: 0.001, Steps: 999\n",
      "... hidden dim: 32, Layers: 4, LR: 0.0001, Steps: 9999\n",
      "... hidden dim: 64, Layers: 1, LR: 0.01, Steps: 99\n",
      "... hidden dim: 64, Layers: 1, LR: 0.001, Steps: 999\n",
      "... hidden dim: 64, Layers: 1, LR: 0.0001, Steps: 9999\n",
      "... hidden dim: 64, Layers: 2, LR: 0.01, Steps: 99\n",
      "... hidden dim: 64, Layers: 2, LR: 0.001, Steps: 999\n",
      "... hidden dim: 64, Layers: 2, LR: 0.0001, Steps: 9999\n",
      "... hidden dim: 64, Layers: 4, LR: 0.01, Steps: 99\n",
      "... hidden dim: 64, Layers: 4, LR: 0.001, Steps: 999\n",
      "... hidden dim: 64, Layers: 4, LR: 0.0001, Steps: 9999\n",
      "Hidden dim: 64, Layers: 4, LR: 0.001, Loss: 1.422892546543153e-05\n",
      "Hidden dim: 32, Layers: 2, LR: 0.001, Loss: 1.7583452063263394e-05\n",
      "Hidden dim: 64, Layers: 2, LR: 0.001, Loss: 0.00014805930550210178\n",
      "Hidden dim: 64, Layers: 2, LR: 0.0001, Loss: 0.00021140110038686544\n",
      "Hidden dim: 32, Layers: 2, LR: 0.0001, Loss: 0.00037158626946620643\n",
      "Hidden dim: 32, Layers: 2, LR: 0.01, Loss: 0.0004842009802814573\n",
      "Hidden dim: 32, Layers: 4, LR: 0.001, Loss: 0.0005662312032654881\n",
      "Hidden dim: 64, Layers: 4, LR: 0.0001, Loss: 0.0006910807569511235\n",
      "Hidden dim: 32, Layers: 4, LR: 0.01, Loss: 0.001130763441324234\n",
      "Hidden dim: 64, Layers: 1, LR: 0.0001, Loss: 0.0014218128053471446\n",
      "Hidden dim: 64, Layers: 4, LR: 0.01, Loss: 0.0024483713787049055\n",
      "Hidden dim: 32, Layers: 1, LR: 0.0001, Loss: 0.002519418252632022\n",
      "Hidden dim: 64, Layers: 2, LR: 0.01, Loss: 0.003138075117021799\n",
      "Hidden dim: 32, Layers: 1, LR: 0.001, Loss: 0.0034782779403030872\n",
      "Hidden dim: 64, Layers: 1, LR: 0.001, Loss: 0.0038084990810602903\n",
      "Hidden dim: 32, Layers: 4, LR: 0.0001, Loss: 0.004026580136269331\n",
      "Hidden dim: 32, Layers: 1, LR: 0.01, Loss: 0.005776212550699711\n",
      "Hidden dim: 64, Layers: 1, LR: 0.01, Loss: 0.007574848365038633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(32, 1, 0.01, 0.005776212550699711),\n",
       " (32, 1, 0.001, 0.0034782779403030872),\n",
       " (32, 1, 0.0001, 0.002519418252632022),\n",
       " (32, 2, 0.01, 0.0004842009802814573),\n",
       " (32, 2, 0.001, 1.7583452063263394e-05),\n",
       " (32, 2, 0.0001, 0.00037158626946620643),\n",
       " (32, 4, 0.01, 0.001130763441324234),\n",
       " (32, 4, 0.001, 0.0005662312032654881),\n",
       " (32, 4, 0.0001, 0.004026580136269331),\n",
       " (64, 1, 0.01, 0.007574848365038633),\n",
       " (64, 1, 0.001, 0.0038084990810602903),\n",
       " (64, 1, 0.0001, 0.0014218128053471446),\n",
       " (64, 2, 0.01, 0.003138075117021799),\n",
       " (64, 2, 0.001, 0.00014805930550210178),\n",
       " (64, 2, 0.0001, 0.00021140110038686544),\n",
       " (64, 4, 0.01, 0.0024483713787049055),\n",
       " (64, 4, 0.001, 1.422892546543153e-05),\n",
       " (64, 4, 0.0001, 0.0006910807569511235)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best(\n",
    "    *,\n",
    "    hidden_dim: list[int],\n",
    "    n_layers: list[int],\n",
    "    lr: list[float],\n",
    "    budget: int = 1,\n",
    "    print_every: int | None = None,\n",
    "):\n",
    "    results: list[tuple[int, int, float, float]] = []\n",
    "    for hd in hidden_dim:\n",
    "        for nl in n_layers:\n",
    "            for l in lr:\n",
    "                n_steps = int(budget // l)\n",
    "                print(f\"... hidden dim: {hd}, Layers: {nl}, LR: {l}, Steps: {n_steps}\")\n",
    "\n",
    "                rnn = RNN(hidden_dim=hd, n_layers=nl)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = torch.optim.Adam(rnn.parameters(), lr=l)\n",
    "\n",
    "                trained_rnn, losses = train(rnn=rnn, n_steps=n_steps, print_every=print_every, optimizer=optimizer, criterion=criterion, seq_length=SEQ_LENGTH)\n",
    "                min_loss = min(losses)\n",
    "\n",
    "                results.append((hd, nl, l, min_loss))\n",
    "\n",
    "    for r in sorted(results, key=lambda x: x[3]):\n",
    "        print(f\"Hidden dim: {r[0]}, Layers: {r[1]}, LR: {r[2]}, Loss: {r[3]}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "find_best(hidden_dim=[32, 64], n_layers=[1, 2, 4], lr=[0.01, 0.001, 0.0001])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61060235a59054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
